# MERL Shopping Dataset



The MERL Shopping Dataset contains 106 videos, each of which is a sequence ~2 minutes long. Each video contains several instances of the following **5 actions:**


  1. **Reach To Shelf**      (reach to shelf)

  2. **Retract From Shelf**  (retract hand from shelf)

  3. **Hand In Shelf**       (extended period with hand in the shelf)

  4. **Inspect Product**     (inspect product while holding it in hand)

  5. **Inspect Shelf**       (look at shelf while not touching and not reaching for the shelf)

## Filenames and Labels

We had 41 people that participated in the shopping videos, with SubjectID = 1,...,41. Each subject is in up to 3 videos, with SessionID = 1, 2, 3. Each of the 106 video files has a filename of the form `xx_yy_crop.mp4`, where xx is the SubjectID and yy is the sessionID.



The ground-truth labels for each video `xx_yy_crop.mp4` are in the Matlab file `xx_yy_label.mat`

The labels for the video are stored in a Matlab cell array variable called `tlabs`. For example:

  - `tlabs{1}` is a K x 2 array that lists the start frame and end frame of all K instances in the video of action type 1 (Reach to Shelf).

  - `tlabs{2}` lists the start and end frame of all instances in the video of action type 2 (Retract from Shelf).

## Data Split
We used the following split in our experiments:

| Data Subset    | SubjectIDs     | Video Files    | Number of videos |
|----------------|----------------|----------------|------------------|
| Training set   | Subjects 1-20  | 1\_1 to 20\_3  | 60 videos        |
| Validation set | Subjects 21-26 | 21\_1 to 26\_3 | 18 videos        |
| Testing set    | Subjects 27-41 | 27\_1 to 41\_2 | 28 videos        |



## Results for This Version of the Dataset
This released version of the dataset is slightly larger (10 more videos) and has more accurate temporal action boundaries than the version of the dataset that we reported on in our paper (citation below).

The following results are based on this released version of the dataset. If the you compare with our results, please cite the paper, but compare with the following numbers (don't compare with the numbers in our paper). 

### Average Precision of Detected Actions

In the table below, we report the Average Precision (AP) results of our entire system using Bidirectional LSTM (B-LSTM) for each action:

| Action             | AP (%)   |
|--------------------|:---------|
| Reach to Shelf     | 92.61    |
| Retract from Shelf | 92.07    |
| Hand in Shelf      | 70.31    |
| Inspect Product    | 73.49    |
| Inspect Shelf      | 80.85    |
|                    |          |
| All Actions        | 81.87 mAP|


Averaged across all actions, our system using one-directional Forward LSTM obtains obtains a mean Average Precision (mAP) of 76.6%, while our system using just Backward LSTM obtains 77.7%. Using Bidirectional LSTM (averaging predictions of forward and backward) improves the result to 81.87% mAP.

For evaluation purposes, we provide the full set of detected actions generated by our algorithm. Each detected action consists of a start frame and end frame, along with a score for the detection. These detected actions for action class X (where X is a numeral from 1 to 5) are stored in the Matlab file `Results/DetectedActions/X.mat`. Here the start and end frame numbers correspond to the row ids in `Results/frame_names.txt`, which consists of filenames for images that were created by extracting frames from the videos at 15 fps (frames per second).

**Evaluation Code:** To obtain the results in the table above from the detected actions and groundtruth labels, we used the code from the following location:

<http://datasets.d2.mpi-inf.mpg.de/MPIICookingActivities/evalCode-1.0.zip>


### Per-Frame Accuracy

We also provide our the predictions that our system output for each video chunk (which consists of 6 frames at 15 fps). The Matlab file `Results/framepreds.mat` contains a matrix of per-chunk scores. Each row in this matrix is a 6 dimensional vector corresponding to scores for the 5 classes + background for each chunk (6 frames) of video. Since we make predictions every 6 frames, the number of rows in this matrix is 1/6 times the total number of 15 fps frames (all of which are in `Results/frame_names.txt`).

When we compare each chunk's highest-scoring class out of 6 classes (5 action classes plus "background") to the groundtruth class of each frame in that chunk, we obtain an overall **per-frame accuracy of 76.4%**.




## How to Cite the Dataset

If you use the MERL Shopping Dataset, please cite this paper:

```
@InProceedings{Singh_2016_CVPR,
author = {Singh, Bharat and Marks, Tim K. and Jones, Michael and Tuzel, Oncel and Shao, Ming},
title = {A Multi-Stream Bi-Directional Recurrent Neural Network for Fine-Grained Action Detection},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2016}
}
```

## Legalese

Â© Copyright 2016 Mitsubishi Electric Research Laboratories (MERL).
All Rights Reserved.

Permission to use these data and their documentation without fee for research and educational purposes, is hereby granted.

IN NO EVENT SHALL MERL BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS DATASET AND ITS DOCUMENTATION, EVEN IF MERL HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.

MERL SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE DATA PROVIDED HEREUNDER ARE ON AN "AS IS" BASIS, AND MERL HAS NO OBLIGATIONS TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.
